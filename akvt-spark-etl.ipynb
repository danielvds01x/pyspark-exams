{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVSy2BqsaSkj"
   },
   "source": [
    "##### General Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Spark Locally\n",
    "import findspark\n",
    "\n",
    "path_to_spark='/usr/local/spark/spark-3.3.0-bin-hadoop3/'\n",
    "findspark.init(path_to_spark)\n",
    "\n",
    "from pyspark.sql import SparkSession, Row, SQLContext, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType\n",
    "import pyspark.sql.functions as F\n",
    "from sqlalchemy import create_engine\n",
    "# from sqlalchemy import Table, Column, Integer, Numeric, String, ForeignKey\n",
    "# from sqlalchemy.types import Boolean, Date, DateTime, Float, Integer, Text, Time, Interval\n",
    "import sqlalchemy.types as T\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \"drivers/mysql-connector-java-8.0.29.jar\") \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"aktv-etl\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Get configuration of this session\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "5wjdDd3tJ74l"
   },
   "outputs": [],
   "source": [
    "def create_dataframe(path: str, table_name: str) -> DataFrame:\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True,\n",
    "                                 quote='\"', escape='\"', multiLine=True)\n",
    "    df.createOrReplaceTempView(table_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_to_rdbms(table_name: str, spark_dataframe: DataFrame , column_schema: dict=None, if_exists: str='replace') -> None:\n",
    "    pd = spark_dataframe.select(\"*\").toPandas()\n",
    "    # To Remove\n",
    "    # print(pd.info())\n",
    "\n",
    "\n",
    "    # Config MySQL\n",
    "    user='root'\n",
    "    port='13306'\n",
    "    host='127.0.0.1'\n",
    "    user='root'\n",
    "    password='pwd123'\n",
    "    database='aktv_dw'\n",
    "\n",
    "    sql_engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}', pool_recycle=3600)\n",
    "    db_connection = sql_engine.connect()\n",
    "\n",
    "    try:\n",
    "        frame = pd.to_sql(table_name, db_connection, if_exists=if_exists, index=False, dtype=column_schema)\n",
    "    except ValueError as vx:\n",
    "        print(vx)\n",
    "    except Exception as ex:   \n",
    "        print(ex)\n",
    "    else:\n",
    "        print(f\"Table {table_name} created/updated successfully.\")\n",
    "    finally:\n",
    "        db_connection.close()\n",
    "        # pd.unpersist()\n",
    "\n",
    "\n",
    "def generate_schema(data_frame: DataFrame) -> dict:\n",
    "        SQL_TYPE_MAP = {\n",
    "        'bool': T.BOOLEAN,\n",
    "        'string': T.TEXT,\n",
    "        'int': T.Integer,\n",
    "        'integer': T.Integer,\n",
    "        'float': T.FLOAT,\n",
    "        'datetime': T.DATETIME,\n",
    "        'date': T.DATE,\n",
    "        'time': T.TIME,\n",
    "        'timedelta': T.Interval,\n",
    "        'timestamp': T.DateTime\n",
    "        }\n",
    "\n",
    "        columns_schema = {}\n",
    "\n",
    "        for field in data_frame.dtypes:\n",
    "                # print(field)\n",
    "                columns_schema[str(field[0])] = str(SQL_TYPE_MAP[field[1]])\n",
    "        \n",
    "        return columns_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table fact_callbacks created/updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Table [callbacks]\n",
    "TABLE_NAME = 'callbacks'\n",
    "TABLE_KIND = 'fact'\n",
    "df = create_dataframe(f\"landing_files/{TABLE_NAME}.csv\", TABLE_NAME)\n",
    "\n",
    "df = df.withColumn(\"data_maptype\", F.from_json(df.data, MapType(StringType(),StringType())))\n",
    "\n",
    "df = df.select(F.col(\"created_at\").alias(\"created_at_original\"), \\\n",
    "        F.explode(df.data_maptype))\n",
    "\n",
    "df = df.groupBy(\"created_at_original\").pivot(\"key\").agg(F.first(\"value\"))\n",
    "\n",
    "df = df.select(F.col(\"created_at_original\"), \\\n",
    "        F.col(\"created_at\"), \\\n",
    "        F.col(\"channel\"), \\\n",
    "        F.col(\"channel_id\"), \\\n",
    "        F.col(\"channel_type\"), \\\n",
    "        F.col(\"cid\"), \\\n",
    "        F.col(\"delete_conversation\"), \\\n",
    "        F.col(\"delete_conversation_channels\"), \\\n",
    "        F.col(\"delete_messages\"), \\\n",
    "        F.col(\"delete_user\"), \\\n",
    "        F.col(\"details\"), \\\n",
    "        F.col(\"hard_delete\"), \\\n",
    "        F.col(\"mark_messages_deleted\"), \\\n",
    "        F.col(\"member\"), \\\n",
    "        F.col(\"members\"), \\\n",
    "        F.col(\"message\"), \\\n",
    "        F.col(\"message_id\"), \\\n",
    "        F.col(\"reaction\"), \\\n",
    "        F.col(\"team\"), \\\n",
    "        F.col(\"total_flags\"), \\\n",
    "        F.col(\"type\"), \\\n",
    "        F.col(\"user\"), \\\n",
    "        F.col(\"watcher_count\"), \\\n",
    "        F.get_json_object(F.col(\"user\"), \"$.id\").alias(\"user_id\")) \\\n",
    "        .withColumn(\"team_id\", F.regexp_extract(\"team\", \"\\\\d+\", 0)) \\\n",
    "\n",
    "df = df.withColumn(\"team_id\", df[\"team_id\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"user_id\", df[\"user_id\"].cast(IntegerType()))\n",
    "\n",
    "# Exporting to MySQL\n",
    "export_to_rdbms(f'{TABLE_KIND}_{TABLE_NAME}', df, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dim_users created/updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Table [users]\n",
    "TABLE_NAME = 'users'\n",
    "TABLE_KIND = 'dim'\n",
    "df = create_dataframe(f\"landing_files/{TABLE_NAME}.csv\", TABLE_NAME)\n",
    "\n",
    "column_name = 'other_sources'\n",
    "df = df.withColumn(column_name,F.expr(f\"substring({column_name}, 2, length({column_name})-2)\"))\n",
    "\n",
    "df = df.select(F.col(\"user_id\"), \\\n",
    "    F.col(\"user_id\"), \\\n",
    "    F.col(\"_fivetran_deleted\"), \\\n",
    "    F.col(\"_fivetran_synced\"), \\\n",
    "    F.col(\"confirmed_at\"), \\\n",
    "    F.col(\"email\"), \\\n",
    "    F.col(\"hashed_password\"), \\\n",
    "    F.col(\"inserted_at\"), \\\n",
    "    F.col(\"other_sources\"), \\\n",
    "    F.col(\"phone_number\"), \\\n",
    "    F.col(\"updated_at\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.id\").alias(\"id\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.source\").alias(\"source\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.table_id\").alias(\"table_id\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.table_name\").alias(\"table_name\")) \\\n",
    "    # .show(1, truncate=False, vertical=True)\n",
    "\n",
    "export_to_rdbms(f'{TABLE_KIND}_{TABLE_NAME}', df, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dim_teams created/updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Table [teams]\n",
    "TABLE_NAME = 'teams'\n",
    "TABLE_KIND = 'dim'\n",
    "df = create_dataframe(f\"landing_files/{TABLE_NAME}.csv\", TABLE_NAME)\n",
    "\n",
    "column_name = 'other_sources'\n",
    "df = df.withColumn(column_name,F.expr(f\"substring({column_name}, 2, length({column_name})-2)\"))\n",
    "\n",
    "df = df.select(F.col(\"team_id\"), \\\n",
    "    F.col(\"_fivetran_deleted\"), \\\n",
    "    F.col(\"_fivetran_synced\"), \\\n",
    "    F.col(\"activity_id\"), \\\n",
    "    F.col(\"collective_name\"), \\\n",
    "    F.col(\"gender\"), \\\n",
    "    F.col(\"inserted_at\"), \\\n",
    "    F.col(\"name\"), \\\n",
    "    F.col(\"other_sources\"), \\\n",
    "    F.col(\"season_id\"), \\\n",
    "    F.col(\"team_level\"), \\\n",
    "    F.col(\"updated_at\"), \\\n",
    "    F.col(\"registration_code\"), \\\n",
    "    F.col(\"school_activity_id\"), \\\n",
    "    F.col(\"status\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.id\").alias(\"id\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.source\").alias(\"source\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.table_id\").alias(\"table_id\"), \\\n",
    "    F.get_json_object(F.col(\"other_sources\"), \"$.table_name\").alias(\"table_name\")) \\\n",
    "    # .show(1, truncate=False, vertical=True)\n",
    "\n",
    "export_to_rdbms(f'{TABLE_KIND}_{TABLE_NAME}', df, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table fact_push_tokens created/updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Table [push_tokens]\n",
    "TABLE_NAME = 'push_tokens'\n",
    "TABLE_KIND = 'fact'\n",
    "df = create_dataframe(f\"landing_files/{TABLE_NAME}.csv\", TABLE_NAME)\n",
    "\n",
    "df = df.select(F.col(\"push_token_id\"), \\\n",
    "    F.col(\"_fivetran_deleted\"), \\\n",
    "    F.col(\"_fivetran_synced\"), \\\n",
    "    F.col(\"expo_push_token\"), \\\n",
    "    F.col(\"inserted_at\"), \\\n",
    "    F.col(\"updated_at\"), \\\n",
    "    F.col(\"user_id\")) \\\n",
    "    .withColumn(\"user_id\", df[\"user_id\"].cast(IntegerType()))\n",
    "    # .show(1, truncate=False, vertical=True)\n",
    "\n",
    "export_to_rdbms(f'{TABLE_KIND}_{TABLE_NAME}', df, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Teste Engenharia de Dados v2_2 - modelo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "696261bc91c04e4df3605ce73073fde8bd8da3b42592b8c5d8186de2e507249e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
